{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Social media sentiment analysis \n",
    "## Part 3b: Topic Modeling using GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Twitter comments for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call data from Part 2\n",
    "tweet_combined_clean=pd.read_csv('./dataset/tweet_combined_clean_v1.csv')\n",
    "samsung_tweet_neg=pd.read_csv('./dataset/samsung_tweet_neg.csv')\n",
    "samsung_tweet_pos=pd.read_csv('./dataset/samsung_tweet_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike VADER, topic modeling requires extensive data cleaning - Edit stopwords to exclude emotional words like 'like','love' and brand names 'Samsung', 'Apple', 'Huawei'\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def text_processer(text):\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    review_text = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "        \n",
    "    # 4. Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.update(['samsung','huawei','apple','http','like','really','want','good','say','love','better','much',\n",
    "                       'day','lol','well','need','could','take','twt','add','maxwinebach','nhlblackhawk','smtshepossa',\n",
    "                       'niantichelp','recognised','view','great','doh','samsungmobile','okay','check','feel','always',\n",
    "                       'yes','stop','even','every','already','u','something','go','see','sure','shit','said','https','com','www','hi','please','co','thanks','one',\n",
    "                       'think','got','also', 'make','know','use', 'would','get','look','never','still','mtshepossa','akinjoshua',\n",
    "                       'pay','using','time','b','c','d','e','f','g','h','i','ksvaljek', 'right', 'used','godissfroot','nhlblackhawks','http',\n",
    "                       'j','k','l','n','m','o','p','q','r','s','t','u','v','w','x','y','z', 'sorry', 'part', 'u', 'let','as', 'saying', 'bit', \n",
    "                       'update', 'techquotesdaily','oh', 'yeah','frecowang', 'bts', 'pak','ok','fuck','come','thing','south','settle','level', \n",
    "                       'took','actually','stand','im','watch', 'jezdez','offby','dirtytesla','universeice','as', 'thank', 'thanks', 'seem', 'seems',\n",
    "                        'way','put','made','thought', 'jack'])\n",
    "    filtered_words=[w for w in words if not w in stop_words]\n",
    "\n",
    "    # 5. Lemmatize words.\n",
    "    lemmed_words = [lemm.lemmatize(i) for i in filtered_words]\n",
    "    \n",
    "    return (\" \".join(lemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling Dirichlet Mixture Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gibbs Sampling Dirichlet Mixture Model (GSDMM) is an “altered” LDA algorithm, showing great results on STTM tasks, that makes the initial assumption: 1 topic ↔️1 document. The words within a document are generated using the same unique topic, and not from a mixture of topics as it was in the original LDA.\n",
    "\n",
    "reference:https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download gsdmm, clone the following git repo from author: https://github.com/rwalk/gsdmm\n",
    "from gsdmm.gsdmm import MovieGroupProcess\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to LDA, GSDMM has a more even splot of documents per topic. Unfortunately, GSDMM is a relatively less used and less maintained model compared to LDA. Its source code did not include performance metric like perplexity and coherence score. Without performance metrics, it is challenging for me to assess whether GSDMM performs better than LDA based on the word clusters. Given the lack of performance metrics, I am unable to optimise the parameters in GSDMM (e.g. number of topics). Therefore, I have set the number of topics (K value) in GSDMM to be same as LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_samsung_clean=tweet_combined_clean[tweet_combined_clean['brand']==0]\n",
    "tweet_samsung_words=tweet_samsung_clean['text'].apply(text_processer)\n",
    "tweet_samsung_words=[d.split() for d in tweet_samsung_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 961 clusters with 2 clusters populated\n",
      "In stage 1: transferred 537 clusters with 2 clusters populated\n",
      "In stage 2: transferred 394 clusters with 2 clusters populated\n",
      "In stage 3: transferred 352 clusters with 2 clusters populated\n",
      "In stage 4: transferred 308 clusters with 2 clusters populated\n",
      "In stage 5: transferred 287 clusters with 2 clusters populated\n",
      "In stage 6: transferred 243 clusters with 2 clusters populated\n",
      "In stage 7: transferred 245 clusters with 2 clusters populated\n",
      "In stage 8: transferred 248 clusters with 2 clusters populated\n",
      "In stage 9: transferred 253 clusters with 2 clusters populated\n",
      "In stage 10: transferred 246 clusters with 2 clusters populated\n",
      "In stage 11: transferred 275 clusters with 2 clusters populated\n",
      "In stage 12: transferred 263 clusters with 2 clusters populated\n",
      "In stage 13: transferred 236 clusters with 2 clusters populated\n",
      "In stage 14: transferred 251 clusters with 2 clusters populated\n",
      "In stage 15: transferred 255 clusters with 2 clusters populated\n",
      "In stage 16: transferred 244 clusters with 2 clusters populated\n",
      "In stage 17: transferred 246 clusters with 2 clusters populated\n",
      "In stage 18: transferred 257 clusters with 2 clusters populated\n",
      "In stage 19: transferred 283 clusters with 2 clusters populated\n",
      "In stage 20: transferred 244 clusters with 2 clusters populated\n",
      "In stage 21: transferred 234 clusters with 2 clusters populated\n",
      "In stage 22: transferred 255 clusters with 2 clusters populated\n",
      "In stage 23: transferred 258 clusters with 2 clusters populated\n",
      "In stage 24: transferred 236 clusters with 2 clusters populated\n",
      "In stage 25: transferred 251 clusters with 2 clusters populated\n",
      "In stage 26: transferred 254 clusters with 2 clusters populated\n",
      "In stage 27: transferred 257 clusters with 2 clusters populated\n",
      "In stage 28: transferred 244 clusters with 2 clusters populated\n",
      "In stage 29: transferred 254 clusters with 2 clusters populated\n",
      "In stage 30: transferred 251 clusters with 2 clusters populated\n",
      "In stage 31: transferred 231 clusters with 2 clusters populated\n",
      "In stage 32: transferred 244 clusters with 2 clusters populated\n",
      "In stage 33: transferred 236 clusters with 2 clusters populated\n",
      "In stage 34: transferred 255 clusters with 2 clusters populated\n",
      "In stage 35: transferred 241 clusters with 2 clusters populated\n",
      "In stage 36: transferred 243 clusters with 2 clusters populated\n",
      "In stage 37: transferred 248 clusters with 2 clusters populated\n",
      "In stage 38: transferred 256 clusters with 2 clusters populated\n",
      "In stage 39: transferred 219 clusters with 2 clusters populated\n",
      "In stage 40: transferred 224 clusters with 2 clusters populated\n",
      "In stage 41: transferred 232 clusters with 2 clusters populated\n",
      "In stage 42: transferred 232 clusters with 2 clusters populated\n",
      "In stage 43: transferred 237 clusters with 2 clusters populated\n",
      "In stage 44: transferred 246 clusters with 2 clusters populated\n",
      "In stage 45: transferred 248 clusters with 2 clusters populated\n",
      "In stage 46: transferred 229 clusters with 2 clusters populated\n",
      "In stage 47: transferred 236 clusters with 2 clusters populated\n",
      "In stage 48: transferred 262 clusters with 2 clusters populated\n",
      "In stage 49: transferred 251 clusters with 2 clusters populated\n",
      "Number of documents per topics : [1142 1296]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [1 0]\n",
      "********************\n",
      "[('galaxy', 130), ('phone', 124), ('iphone', 72), ('note', 72), ('samsungmobileus', 49), ('android', 40), ('plus', 40), ('red', 35), ('mobile', 28), ('samsunguk', 28), ('oneplus', 27), ('black', 26), ('pro', 24), ('price', 24), ('camera', 22), [...]]\n",
      "[('tv', 227), ('phone', 126), ('galaxy', 93), ('ad', 83), ('app', 56), ('iphone', 56), ('android', 46), ('new', 44), ('smart', 43), ('oneplus', 40), ('play', 35), ('channel', 34), ('year', 27), ('work', 26), ('series', 24), [...]]\n"
     ]
    }
   ],
   "source": [
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "# K = number of potential topic (which we don't know a priori).In a real case we are not aware of the exact number of topic so we want to choose a higher value. Theoretically, GSDMM should empty useless clusters and eventually find the exact number of cluster.\n",
    "# alpha & beta: kept the default parameters (which work well for several datasets). However, one might want to tune them to improve its topic allocation regarding the completeness and homogeneity of the clusters.\n",
    "# n_iters = number of iterations\n",
    "mgp = MovieGroupProcess(K=2, alpha=0.1, beta=0.1, n_iters=50)\n",
    "\n",
    "vocab = set(x for doc in tweet_samsung_words for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(tweet_samsung_words, n_terms)\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "top_words=[]\n",
    "\n",
    "for i in range(len(mgp.cluster_word_distribution)):\n",
    "    top_words=Counter(mgp.cluster_word_distribution[i]).most_common(15)\n",
    "    top_words.append(top_words)\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 852 clusters with 2 clusters populated\n",
      "In stage 1: transferred 419 clusters with 2 clusters populated\n",
      "In stage 2: transferred 343 clusters with 2 clusters populated\n",
      "In stage 3: transferred 320 clusters with 2 clusters populated\n",
      "In stage 4: transferred 323 clusters with 2 clusters populated\n",
      "In stage 5: transferred 310 clusters with 2 clusters populated\n",
      "In stage 6: transferred 281 clusters with 2 clusters populated\n",
      "In stage 7: transferred 282 clusters with 2 clusters populated\n",
      "In stage 8: transferred 286 clusters with 2 clusters populated\n",
      "In stage 9: transferred 275 clusters with 2 clusters populated\n",
      "In stage 10: transferred 278 clusters with 2 clusters populated\n",
      "In stage 11: transferred 253 clusters with 2 clusters populated\n",
      "In stage 12: transferred 252 clusters with 2 clusters populated\n",
      "In stage 13: transferred 283 clusters with 2 clusters populated\n",
      "In stage 14: transferred 261 clusters with 2 clusters populated\n",
      "In stage 15: transferred 264 clusters with 2 clusters populated\n",
      "In stage 16: transferred 256 clusters with 2 clusters populated\n",
      "In stage 17: transferred 290 clusters with 2 clusters populated\n",
      "In stage 18: transferred 259 clusters with 2 clusters populated\n",
      "In stage 19: transferred 246 clusters with 2 clusters populated\n",
      "In stage 20: transferred 279 clusters with 2 clusters populated\n",
      "In stage 21: transferred 244 clusters with 2 clusters populated\n",
      "In stage 22: transferred 230 clusters with 2 clusters populated\n",
      "In stage 23: transferred 241 clusters with 2 clusters populated\n",
      "In stage 24: transferred 217 clusters with 2 clusters populated\n",
      "In stage 25: transferred 238 clusters with 2 clusters populated\n",
      "In stage 26: transferred 243 clusters with 2 clusters populated\n",
      "In stage 27: transferred 231 clusters with 2 clusters populated\n",
      "In stage 28: transferred 229 clusters with 2 clusters populated\n",
      "In stage 29: transferred 225 clusters with 2 clusters populated\n",
      "In stage 30: transferred 224 clusters with 2 clusters populated\n",
      "In stage 31: transferred 218 clusters with 2 clusters populated\n",
      "In stage 32: transferred 222 clusters with 2 clusters populated\n",
      "In stage 33: transferred 246 clusters with 2 clusters populated\n",
      "In stage 34: transferred 248 clusters with 2 clusters populated\n",
      "In stage 35: transferred 232 clusters with 2 clusters populated\n",
      "In stage 36: transferred 240 clusters with 2 clusters populated\n",
      "In stage 37: transferred 245 clusters with 2 clusters populated\n",
      "In stage 38: transferred 249 clusters with 2 clusters populated\n",
      "In stage 39: transferred 221 clusters with 2 clusters populated\n",
      "In stage 40: transferred 227 clusters with 2 clusters populated\n",
      "In stage 41: transferred 222 clusters with 2 clusters populated\n",
      "In stage 42: transferred 216 clusters with 2 clusters populated\n",
      "In stage 43: transferred 242 clusters with 2 clusters populated\n",
      "In stage 44: transferred 230 clusters with 2 clusters populated\n",
      "In stage 45: transferred 236 clusters with 2 clusters populated\n",
      "In stage 46: transferred 249 clusters with 2 clusters populated\n",
      "In stage 47: transferred 241 clusters with 2 clusters populated\n",
      "In stage 48: transferred 229 clusters with 2 clusters populated\n",
      "In stage 49: transferred 241 clusters with 2 clusters populated\n",
      "Number of documents per topics : [1061 1150]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [1 0]\n",
      "********************\n",
      "[('music', 38), ('app', 36), ('iphone', 31), ('store', 27), ('phone', 26), ('new', 22), ('help', 22), ('microsoft', 21), ('iamtbotouch', 19), ('android', 19), ('available', 18), ('pro', 18), ('product', 18), ('id', 16), ('ipad', 16), [...]]\n",
      "[('music', 213), ('spotify', 68), ('tv', 50), ('song', 36), ('playlist', 24), ('people', 19), ('eye', 19), ('card', 19), ('app', 18), ('google', 17), ('big', 17), ('stream', 17), ('tidal', 17), ('adam', 17), ('link', 16), [...]]\n"
     ]
    }
   ],
   "source": [
    "tweet_apple_clean=tweet_combined_clean[tweet_combined_clean['brand']==1]\n",
    "tweet_apple_words=tweet_apple_clean['text'].apply(text_processer)\n",
    "tweet_apple_words=[d.split() for d in tweet_apple_words]\n",
    "\n",
    "mgp = MovieGroupProcess(K=2, alpha=0.1, beta=0.1, n_iters=50)\n",
    "vocab = set(x for doc in tweet_apple_words for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(tweet_apple_words, n_terms)\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "top_words=[]\n",
    "\n",
    "for i in range(len(mgp.cluster_word_distribution)):\n",
    "    top_words=Counter(mgp.cluster_word_distribution[i]).most_common(15)\n",
    "    top_words.append(top_words)\n",
    "    print(top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huawei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 936 clusters with 2 clusters populated\n",
      "In stage 1: transferred 452 clusters with 2 clusters populated\n",
      "In stage 2: transferred 316 clusters with 2 clusters populated\n",
      "In stage 3: transferred 297 clusters with 2 clusters populated\n",
      "In stage 4: transferred 326 clusters with 2 clusters populated\n",
      "In stage 5: transferred 292 clusters with 2 clusters populated\n",
      "In stage 6: transferred 250 clusters with 2 clusters populated\n",
      "In stage 7: transferred 238 clusters with 2 clusters populated\n",
      "In stage 8: transferred 256 clusters with 2 clusters populated\n",
      "In stage 9: transferred 215 clusters with 2 clusters populated\n",
      "In stage 10: transferred 229 clusters with 2 clusters populated\n",
      "In stage 11: transferred 247 clusters with 2 clusters populated\n",
      "In stage 12: transferred 248 clusters with 2 clusters populated\n",
      "In stage 13: transferred 209 clusters with 2 clusters populated\n",
      "In stage 14: transferred 217 clusters with 2 clusters populated\n",
      "In stage 15: transferred 211 clusters with 2 clusters populated\n",
      "In stage 16: transferred 214 clusters with 2 clusters populated\n",
      "In stage 17: transferred 204 clusters with 2 clusters populated\n",
      "In stage 18: transferred 211 clusters with 2 clusters populated\n",
      "In stage 19: transferred 216 clusters with 2 clusters populated\n",
      "In stage 20: transferred 224 clusters with 2 clusters populated\n",
      "In stage 21: transferred 212 clusters with 2 clusters populated\n",
      "In stage 22: transferred 243 clusters with 2 clusters populated\n",
      "In stage 23: transferred 249 clusters with 2 clusters populated\n",
      "In stage 24: transferred 209 clusters with 2 clusters populated\n",
      "In stage 25: transferred 208 clusters with 2 clusters populated\n",
      "In stage 26: transferred 193 clusters with 2 clusters populated\n",
      "In stage 27: transferred 190 clusters with 2 clusters populated\n",
      "In stage 28: transferred 190 clusters with 2 clusters populated\n",
      "In stage 29: transferred 200 clusters with 2 clusters populated\n",
      "In stage 30: transferred 205 clusters with 2 clusters populated\n",
      "In stage 31: transferred 201 clusters with 2 clusters populated\n",
      "In stage 32: transferred 196 clusters with 2 clusters populated\n",
      "In stage 33: transferred 216 clusters with 2 clusters populated\n",
      "In stage 34: transferred 204 clusters with 2 clusters populated\n",
      "In stage 35: transferred 202 clusters with 2 clusters populated\n",
      "In stage 36: transferred 200 clusters with 2 clusters populated\n",
      "In stage 37: transferred 228 clusters with 2 clusters populated\n",
      "In stage 38: transferred 206 clusters with 2 clusters populated\n",
      "In stage 39: transferred 206 clusters with 2 clusters populated\n",
      "In stage 40: transferred 202 clusters with 2 clusters populated\n",
      "In stage 41: transferred 197 clusters with 2 clusters populated\n",
      "In stage 42: transferred 186 clusters with 2 clusters populated\n",
      "In stage 43: transferred 176 clusters with 2 clusters populated\n",
      "In stage 44: transferred 191 clusters with 2 clusters populated\n",
      "In stage 45: transferred 185 clusters with 2 clusters populated\n",
      "In stage 46: transferred 180 clusters with 2 clusters populated\n",
      "In stage 47: transferred 186 clusters with 2 clusters populated\n",
      "In stage 48: transferred 185 clusters with 2 clusters populated\n",
      "In stage 49: transferred 201 clusters with 2 clusters populated\n",
      "Number of documents per topics : [1268 1136]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [0 1]\n",
      "********************\n",
      "[('china', 224), ('chinese', 113), ('canada', 104), ('mask', 102), ('globalnews', 92), ('ccp', 87), ('u', 69), ('company', 66), ('uk', 42), ('phone', 41), ('world', 41), ('technology', 39), ('country', 38), ('canadian', 38), ('sunlorrie', 38), [...]]\n",
      "[('china', 79), ('europe', 79), ('leeszla', 66), ('people', 65), ('phone', 63), ('pro', 57), ('mike', 40), ('psyberchic', 36), ('u', 35), ('company', 34), ('chinese', 33), ('imc', 32), ('huaweimobile', 31), ('ttsampaio', 30), ('covid', 26), [...]]\n"
     ]
    }
   ],
   "source": [
    "tweet_huawei_clean=tweet_combined_clean[tweet_combined_clean['brand']==2]\n",
    "tweet_huawei_words=tweet_huawei_clean['text'].apply(text_processer)\n",
    "tweet_huawei_words=[d.split() for d in tweet_huawei_words]\n",
    "\n",
    "mgp = MovieGroupProcess(K=2, alpha=0.1, beta=0.1, n_iters=50)\n",
    "vocab = set(x for doc in tweet_huawei_words for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(tweet_huawei_words, n_terms)\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "top_words=[]\n",
    "\n",
    "for i in range(len(mgp.cluster_word_distribution)):\n",
    "    top_words=Counter(mgp.cluster_word_distribution[i]).most_common(15)\n",
    "    top_words.append(top_words)\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
