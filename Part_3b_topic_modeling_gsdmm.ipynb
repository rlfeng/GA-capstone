{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Social media sentiment analysis \n",
    "## Part 4: Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\frl\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas.util.testing as tm\n",
    "import spacy\n",
    "\n",
    "# Gensim libraries\n",
    "from gensim import corpora\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import CoherenceModel, Word2Vec, LsiModel, KeyedVectors, fasttext,LdaModel\n",
    "from gensim import matutils\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Enable logging for gensim - optional but important\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Twitter comments for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call data from Part 2\n",
    "tweet_combined_clean=pd.read_csv('./dataset/tweet_combined_clean_v1.csv')\n",
    "samsung_tweet_neg=pd.read_csv('./dataset/samsung_tweet_neg.csv')\n",
    "samsung_tweet_pos=pd.read_csv('./dataset/samsung_tweet_pos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike VADER, topic modeling requires extensive data cleaning - Edit stopwords to exclude emotional words like 'like','love' and brand names 'Samsung', 'Apple', 'Huawei'\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def text_processer(text):\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    review_text = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "        \n",
    "    # 4. Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.update(['samsung','huawei','apple','http','like','really','want','good','say','love','better','much',\n",
    "                       'day','lol','well','need','could','take','twt','add','maxwinebach','nhlblackhawk','smtshepossa',\n",
    "                       'niantichelp','recognised','view','great','doh','samsungmobile','okay','check','feel','always',\n",
    "                       'yes','stop','even','every','already','u','something','go','see','sure','shit','said','https','com','www','hi','please','co','thanks','one',\n",
    "                       'think','got','also', 'make','know','use', 'would','get','look','never','still','mtshepossa','akinjoshua',\n",
    "                       'pay','using','time','b','c','d','e','f','g','h','i','ksvaljek', 'right', 'used','godissfroot','nhlblackhawks','http',\n",
    "                       'j','k','l','n','m','o','p','q','r','s','t','u','v','w','x','y','z', 'sorry', 'part', 'u', 'let','as', 'saying', 'bit', \n",
    "                       'update', 'techquotesdaily','oh', 'yeah','frecowang', 'bts', 'pak','ok','fuck','come','thing','south','settle','level', \n",
    "                       'took','actually','stand','im','watch', 'jezdez','offby','dirtytesla','universeice','as', 'thank', 'thanks', 'seem', 'seems',\n",
    "                        'way','put','made','thought', 'jack'])\n",
    "    filtered_words=[w for w in words if not w in stop_words]\n",
    "\n",
    "    # 5. Lemmatize words.\n",
    "    lemmed_words = [lemm.lemmatize(i) for i in filtered_words]\n",
    "    \n",
    "    return (\" \".join(lemmed_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampling Dirichlet Mixture Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gibbs Sampling Dirichlet Mixture Model (GSDMM) is an “altered” LDA algorithm, showing great results on STTM tasks, that makes the initial assumption: 1 topic ↔️1 document. The words within a document are generated using the same unique topic, and not from a mixture of topics as it was in the original LDA.\n",
    "\n",
    "reference:https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm.gsdmm import MovieGroupProcess\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_samsung_clean=tweet_combined_clean[tweet_combined_clean['brand']==0]\n",
    "tweet_samsung_words=tweet_samsung_clean['text'].apply(text_processer)\n",
    "tweet_samsung_words=[d.split() for d in tweet_samsung_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 961 clusters with 2 clusters populated\n",
      "In stage 1: transferred 537 clusters with 2 clusters populated\n",
      "In stage 2: transferred 394 clusters with 2 clusters populated\n",
      "In stage 3: transferred 352 clusters with 2 clusters populated\n",
      "In stage 4: transferred 308 clusters with 2 clusters populated\n",
      "In stage 5: transferred 287 clusters with 2 clusters populated\n",
      "In stage 6: transferred 243 clusters with 2 clusters populated\n",
      "In stage 7: transferred 245 clusters with 2 clusters populated\n",
      "In stage 8: transferred 248 clusters with 2 clusters populated\n",
      "In stage 9: transferred 253 clusters with 2 clusters populated\n",
      "In stage 10: transferred 246 clusters with 2 clusters populated\n",
      "In stage 11: transferred 275 clusters with 2 clusters populated\n",
      "In stage 12: transferred 263 clusters with 2 clusters populated\n",
      "In stage 13: transferred 236 clusters with 2 clusters populated\n",
      "In stage 14: transferred 251 clusters with 2 clusters populated\n",
      "In stage 15: transferred 255 clusters with 2 clusters populated\n",
      "In stage 16: transferred 244 clusters with 2 clusters populated\n",
      "In stage 17: transferred 246 clusters with 2 clusters populated\n",
      "In stage 18: transferred 257 clusters with 2 clusters populated\n",
      "In stage 19: transferred 283 clusters with 2 clusters populated\n",
      "In stage 20: transferred 244 clusters with 2 clusters populated\n",
      "In stage 21: transferred 234 clusters with 2 clusters populated\n",
      "In stage 22: transferred 255 clusters with 2 clusters populated\n",
      "In stage 23: transferred 258 clusters with 2 clusters populated\n",
      "In stage 24: transferred 236 clusters with 2 clusters populated\n",
      "In stage 25: transferred 251 clusters with 2 clusters populated\n",
      "In stage 26: transferred 254 clusters with 2 clusters populated\n",
      "In stage 27: transferred 257 clusters with 2 clusters populated\n",
      "In stage 28: transferred 244 clusters with 2 clusters populated\n",
      "In stage 29: transferred 254 clusters with 2 clusters populated\n",
      "In stage 30: transferred 251 clusters with 2 clusters populated\n",
      "In stage 31: transferred 231 clusters with 2 clusters populated\n",
      "In stage 32: transferred 244 clusters with 2 clusters populated\n",
      "In stage 33: transferred 236 clusters with 2 clusters populated\n",
      "In stage 34: transferred 255 clusters with 2 clusters populated\n",
      "In stage 35: transferred 241 clusters with 2 clusters populated\n",
      "In stage 36: transferred 243 clusters with 2 clusters populated\n",
      "In stage 37: transferred 248 clusters with 2 clusters populated\n",
      "In stage 38: transferred 256 clusters with 2 clusters populated\n",
      "In stage 39: transferred 219 clusters with 2 clusters populated\n",
      "In stage 40: transferred 224 clusters with 2 clusters populated\n",
      "In stage 41: transferred 232 clusters with 2 clusters populated\n",
      "In stage 42: transferred 232 clusters with 2 clusters populated\n",
      "In stage 43: transferred 237 clusters with 2 clusters populated\n",
      "In stage 44: transferred 246 clusters with 2 clusters populated\n",
      "In stage 45: transferred 248 clusters with 2 clusters populated\n",
      "In stage 46: transferred 229 clusters with 2 clusters populated\n",
      "In stage 47: transferred 236 clusters with 2 clusters populated\n",
      "In stage 48: transferred 262 clusters with 2 clusters populated\n",
      "In stage 49: transferred 251 clusters with 2 clusters populated\n",
      "Number of documents per topics : [1142 1296]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [1 0]\n",
      "********************\n",
      "[('galaxy', 130), ('phone', 124), ('iphone', 72), ('note', 72), ('samsungmobileus', 49), ('android', 40), ('plus', 40), ('red', 35), ('mobile', 28), ('samsunguk', 28), ('oneplus', 27), ('black', 26), ('pro', 24), ('price', 24), ('camera', 22), [...]]\n",
      "[('tv', 227), ('phone', 126), ('galaxy', 93), ('ad', 83), ('app', 56), ('iphone', 56), ('android', 46), ('new', 44), ('smart', 43), ('oneplus', 40), ('play', 35), ('channel', 34), ('year', 27), ('work', 26), ('series', 24), [...]]\n"
     ]
    }
   ],
   "source": [
    "# Init of the Gibbs Sampling Dirichlet Mixture Model algorithm\n",
    "# K = number of potential topic (which we don't know a priori).In a real case we are not aware of the exact number of topic so we want to choose a higher value. Theoretically, GSDMM should empty useless clusters and eventually find the exact number of cluster.\n",
    "# alpha & beta: kept the default parameters (which work well for several datasets). However, one might want to tune them to improve its topic allocation regarding the completeness and homogeneity of the clusters.\n",
    "# n_iters = number of iterations\n",
    "mgp = MovieGroupProcess(K=2, alpha=0.1, beta=0.1, n_iters=50)\n",
    "\n",
    "vocab = set(x for doc in tweet_samsung_words for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(tweet_samsung_words, n_terms)\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "top_words=[]\n",
    "\n",
    "for i in range(len(mgp.cluster_word_distribution)):\n",
    "    top_words=Counter(mgp.cluster_word_distribution[i]).most_common(15)\n",
    "    top_words.append(top_words)\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is manually generated and the topics are assigned by me. However, I could not distinguish the differences among many word clusters and ended up with similar topics for different word clusters.\n",
    "1) phones\n",
    "2) samsung phone product\n",
    "3) samsung phone\n",
    "4) samsung tv\n",
    "5) samsung phone\n",
    "6) samsung advertisements\n",
    "7) samsung phone\n",
    "8) phone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 852 clusters with 2 clusters populated\n",
      "In stage 1: transferred 419 clusters with 2 clusters populated\n",
      "In stage 2: transferred 343 clusters with 2 clusters populated\n",
      "In stage 3: transferred 320 clusters with 2 clusters populated\n",
      "In stage 4: transferred 323 clusters with 2 clusters populated\n",
      "In stage 5: transferred 310 clusters with 2 clusters populated\n",
      "In stage 6: transferred 281 clusters with 2 clusters populated\n",
      "In stage 7: transferred 282 clusters with 2 clusters populated\n",
      "In stage 8: transferred 286 clusters with 2 clusters populated\n",
      "In stage 9: transferred 275 clusters with 2 clusters populated\n",
      "In stage 10: transferred 278 clusters with 2 clusters populated\n",
      "In stage 11: transferred 253 clusters with 2 clusters populated\n",
      "In stage 12: transferred 252 clusters with 2 clusters populated\n",
      "In stage 13: transferred 283 clusters with 2 clusters populated\n",
      "In stage 14: transferred 261 clusters with 2 clusters populated\n",
      "In stage 15: transferred 264 clusters with 2 clusters populated\n",
      "In stage 16: transferred 256 clusters with 2 clusters populated\n",
      "In stage 17: transferred 290 clusters with 2 clusters populated\n",
      "In stage 18: transferred 259 clusters with 2 clusters populated\n",
      "In stage 19: transferred 246 clusters with 2 clusters populated\n",
      "In stage 20: transferred 279 clusters with 2 clusters populated\n",
      "In stage 21: transferred 244 clusters with 2 clusters populated\n",
      "In stage 22: transferred 230 clusters with 2 clusters populated\n",
      "In stage 23: transferred 241 clusters with 2 clusters populated\n",
      "In stage 24: transferred 217 clusters with 2 clusters populated\n",
      "In stage 25: transferred 238 clusters with 2 clusters populated\n",
      "In stage 26: transferred 243 clusters with 2 clusters populated\n",
      "In stage 27: transferred 231 clusters with 2 clusters populated\n",
      "In stage 28: transferred 229 clusters with 2 clusters populated\n",
      "In stage 29: transferred 225 clusters with 2 clusters populated\n",
      "In stage 30: transferred 224 clusters with 2 clusters populated\n",
      "In stage 31: transferred 218 clusters with 2 clusters populated\n",
      "In stage 32: transferred 222 clusters with 2 clusters populated\n",
      "In stage 33: transferred 246 clusters with 2 clusters populated\n",
      "In stage 34: transferred 248 clusters with 2 clusters populated\n",
      "In stage 35: transferred 232 clusters with 2 clusters populated\n",
      "In stage 36: transferred 240 clusters with 2 clusters populated\n",
      "In stage 37: transferred 245 clusters with 2 clusters populated\n",
      "In stage 38: transferred 249 clusters with 2 clusters populated\n",
      "In stage 39: transferred 221 clusters with 2 clusters populated\n",
      "In stage 40: transferred 227 clusters with 2 clusters populated\n",
      "In stage 41: transferred 222 clusters with 2 clusters populated\n",
      "In stage 42: transferred 216 clusters with 2 clusters populated\n",
      "In stage 43: transferred 242 clusters with 2 clusters populated\n",
      "In stage 44: transferred 230 clusters with 2 clusters populated\n",
      "In stage 45: transferred 236 clusters with 2 clusters populated\n",
      "In stage 46: transferred 249 clusters with 2 clusters populated\n",
      "In stage 47: transferred 241 clusters with 2 clusters populated\n",
      "In stage 48: transferred 229 clusters with 2 clusters populated\n",
      "In stage 49: transferred 241 clusters with 2 clusters populated\n",
      "Number of documents per topics : [1061 1150]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [1 0]\n",
      "********************\n",
      "[('music', 38), ('app', 36), ('iphone', 31), ('store', 27), ('phone', 26), ('new', 22), ('help', 22), ('microsoft', 21), ('iamtbotouch', 19), ('android', 19), ('available', 18), ('pro', 18), ('product', 18), ('id', 16), ('ipad', 16), [...]]\n",
      "[('music', 213), ('spotify', 68), ('tv', 50), ('song', 36), ('playlist', 24), ('people', 19), ('eye', 19), ('card', 19), ('app', 18), ('google', 17), ('big', 17), ('stream', 17), ('tidal', 17), ('adam', 17), ('link', 16), [...]]\n"
     ]
    }
   ],
   "source": [
    "tweet_apple_clean=tweet_combined_clean[tweet_combined_clean['brand']==1]\n",
    "tweet_apple_words=tweet_apple_clean['text'].apply(text_processer)\n",
    "tweet_apple_words=[d.split() for d in tweet_apple_words]\n",
    "\n",
    "mgp = MovieGroupProcess(K=2, alpha=0.1, beta=0.1, n_iters=50)\n",
    "vocab = set(x for doc in tweet_apple_words for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(tweet_apple_words, n_terms)\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "top_words=[]\n",
    "\n",
    "for i in range(len(mgp.cluster_word_distribution)):\n",
    "    top_words=Counter(mgp.cluster_word_distribution[i]).most_common(15)\n",
    "    top_words.append(top_words)\n",
    "    print(top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is manually generated. The topics are assigned by me but there are some that I could not come up with a feasible topic.\n",
    "1) ipad\n",
    "2) nil\n",
    "3) iphone\n",
    "4) Apple store\n",
    "5) Music\n",
    "6) Software\n",
    "7) app\n",
    "8) nil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huawei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 936 clusters with 2 clusters populated\n",
      "In stage 1: transferred 452 clusters with 2 clusters populated\n",
      "In stage 2: transferred 316 clusters with 2 clusters populated\n",
      "In stage 3: transferred 297 clusters with 2 clusters populated\n",
      "In stage 4: transferred 326 clusters with 2 clusters populated\n",
      "In stage 5: transferred 292 clusters with 2 clusters populated\n",
      "In stage 6: transferred 250 clusters with 2 clusters populated\n",
      "In stage 7: transferred 238 clusters with 2 clusters populated\n",
      "In stage 8: transferred 256 clusters with 2 clusters populated\n",
      "In stage 9: transferred 215 clusters with 2 clusters populated\n",
      "In stage 10: transferred 229 clusters with 2 clusters populated\n",
      "In stage 11: transferred 247 clusters with 2 clusters populated\n",
      "In stage 12: transferred 248 clusters with 2 clusters populated\n",
      "In stage 13: transferred 209 clusters with 2 clusters populated\n",
      "In stage 14: transferred 217 clusters with 2 clusters populated\n",
      "In stage 15: transferred 211 clusters with 2 clusters populated\n",
      "In stage 16: transferred 214 clusters with 2 clusters populated\n",
      "In stage 17: transferred 204 clusters with 2 clusters populated\n",
      "In stage 18: transferred 211 clusters with 2 clusters populated\n",
      "In stage 19: transferred 216 clusters with 2 clusters populated\n",
      "In stage 20: transferred 224 clusters with 2 clusters populated\n",
      "In stage 21: transferred 212 clusters with 2 clusters populated\n",
      "In stage 22: transferred 243 clusters with 2 clusters populated\n",
      "In stage 23: transferred 249 clusters with 2 clusters populated\n",
      "In stage 24: transferred 209 clusters with 2 clusters populated\n",
      "In stage 25: transferred 208 clusters with 2 clusters populated\n",
      "In stage 26: transferred 193 clusters with 2 clusters populated\n",
      "In stage 27: transferred 190 clusters with 2 clusters populated\n",
      "In stage 28: transferred 190 clusters with 2 clusters populated\n",
      "In stage 29: transferred 200 clusters with 2 clusters populated\n",
      "In stage 30: transferred 205 clusters with 2 clusters populated\n",
      "In stage 31: transferred 201 clusters with 2 clusters populated\n",
      "In stage 32: transferred 196 clusters with 2 clusters populated\n",
      "In stage 33: transferred 216 clusters with 2 clusters populated\n",
      "In stage 34: transferred 204 clusters with 2 clusters populated\n",
      "In stage 35: transferred 202 clusters with 2 clusters populated\n",
      "In stage 36: transferred 200 clusters with 2 clusters populated\n",
      "In stage 37: transferred 228 clusters with 2 clusters populated\n",
      "In stage 38: transferred 206 clusters with 2 clusters populated\n",
      "In stage 39: transferred 206 clusters with 2 clusters populated\n",
      "In stage 40: transferred 202 clusters with 2 clusters populated\n",
      "In stage 41: transferred 197 clusters with 2 clusters populated\n",
      "In stage 42: transferred 186 clusters with 2 clusters populated\n",
      "In stage 43: transferred 176 clusters with 2 clusters populated\n",
      "In stage 44: transferred 191 clusters with 2 clusters populated\n",
      "In stage 45: transferred 185 clusters with 2 clusters populated\n",
      "In stage 46: transferred 180 clusters with 2 clusters populated\n",
      "In stage 47: transferred 186 clusters with 2 clusters populated\n",
      "In stage 48: transferred 185 clusters with 2 clusters populated\n",
      "In stage 49: transferred 201 clusters with 2 clusters populated\n",
      "Number of documents per topics : [1268 1136]\n",
      "********************\n",
      "Most important clusters (by number of docs inside): [0 1]\n",
      "********************\n",
      "[('china', 224), ('chinese', 113), ('canada', 104), ('mask', 102), ('globalnews', 92), ('ccp', 87), ('u', 69), ('company', 66), ('uk', 42), ('phone', 41), ('world', 41), ('technology', 39), ('country', 38), ('canadian', 38), ('sunlorrie', 38), [...]]\n",
      "[('china', 79), ('europe', 79), ('leeszla', 66), ('people', 65), ('phone', 63), ('pro', 57), ('mike', 40), ('psyberchic', 36), ('u', 35), ('company', 34), ('chinese', 33), ('imc', 32), ('huaweimobile', 31), ('ttsampaio', 30), ('covid', 26), [...]]\n"
     ]
    }
   ],
   "source": [
    "tweet_huawei_clean=tweet_combined_clean[tweet_combined_clean['brand']==2]\n",
    "tweet_huawei_words=tweet_huawei_clean['text'].apply(text_processer)\n",
    "tweet_huawei_words=[d.split() for d in tweet_huawei_words]\n",
    "\n",
    "mgp = MovieGroupProcess(K=2, alpha=0.1, beta=0.1, n_iters=50)\n",
    "vocab = set(x for doc in tweet_huawei_words for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(tweet_huawei_words, n_terms)\n",
    "\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topics :', doc_count)\n",
    "print('*'*20)\n",
    "\n",
    "# Topics sorted by document inside\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "\n",
    "top_words=[]\n",
    "\n",
    "for i in range(len(mgp.cluster_word_distribution)):\n",
    "    top_words=Counter(mgp.cluster_word_distribution[i]).most_common(15)\n",
    "    top_words.append(top_words)\n",
    "    print(top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list is manually generated. The topics are assigned by me but there are some that I could not come up with a feasible topic.\n",
    "1) China communist party\n",
    "2) trade war\n",
    "3) China on global news\n",
    "4) nil\n",
    "5) nil\n",
    "6) Huawei product\n",
    "7) Covid-19 virus\n",
    "8) phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
