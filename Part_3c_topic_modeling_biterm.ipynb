{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Social media sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3c: Topic Modeling using Biterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download biterm model from following github\n",
    "#!pip install git+git://github.com/markoarnauto/biterm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "from biterm.btm import oBTM\n",
    "from biterm.utility import vec_to_biterms, topic_summuary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call data from Part 2\n",
    "tweet_combined_clean=pd.read_csv('./dataset/tweet_combined_clean_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike VADER, topic modeling requires extensive data cleaning - Edit stopwords to exclude emotional words like 'like','love' and brand names 'Samsung', 'Apple', 'Huawei'\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def text_processer(text):\n",
    "    \n",
    "    # 1. Remove HTML.\n",
    "    review_text = BeautifulSoup(text).get_text()\n",
    "    \n",
    "    # 2. Remove non-letters.\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "        \n",
    "    # 4. Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.update(['samsung','huawei','apple','http','like','really','want','good','say','love','better','much',\n",
    "                       'day','lol','well','need','could','take','twt','add','maxwinebach','nhlblackhawk','smtshepossa',\n",
    "                       'niantichelp','recognised','view','great','doh','samsungmobile','okay','check','feel','always',\n",
    "                       'yes','stop','even','every','already','u','something','go','see','sure','shit','said','https','com','www','hi','please','co','thanks','one',\n",
    "                       'think','got','also', 'make','know','use', 'would','get','look','never','still','mtshepossa','akinjoshua',\n",
    "                       'pay','using','time','b','c','d','e','f','g','h','i','ksvaljek', 'right', 'used','godissfroot','nhlblackhawks','http',\n",
    "                       'j','k','l','n','m','o','p','q','r','s','t','u','v','w','x','y','z', 'sorry', 'part', 'u', 'let','as', 'saying', 'bit', \n",
    "                       'update', 'techquotesdaily','oh', 'yeah','frecowang', 'bts', 'pak','ok','fuck','come','thing','south','settle','level', \n",
    "                       'took','actually','stand','im','watch', 'jezdez','offby','dirtytesla','universeice','as', 'thank', 'thanks', 'seem', 'seems',\n",
    "                        'way','put','made','thought', 'jack'])\n",
    "    filtered_words=[w for w in words if not w in stop_words]\n",
    "\n",
    "    # 5. Lemmatize words.\n",
    "    lemmed_words = [lemm.lemmatize(i) for i in filtered_words]\n",
    "    \n",
    "    return (\" \".join(lemmed_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise both types of vectorizer for comparison\n",
    "cv = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,\n",
    "                            token_pattern='[a-zA-Z0-9]{3,}') #set number of characters to be more than 3 per word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samsung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_samsung_clean=tweet_combined_clean[tweet_combined_clean['brand']==0]\n",
    "tweet_samsung_words=tweet_samsung_clean['text'].apply(text_processer)\n",
    "samsung_cv=cv.fit_transform(tweet_samsung_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 50.04it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 35.78it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 53.45it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.31it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.53it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.74it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.82it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 40.01it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 55.67it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.78it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 46.10it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 50.95it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.69it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 55.09it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 52.48it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 39.70it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 29.46it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 41.13it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 33.42it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 40.04it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 42.43it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 41.24it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 35.96it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 33.06it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 136.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Topic coherence ..\n",
      "Topic 0 | Coherence=-260.11 | Top words= samsungmobileus samsunguk huaweiuk huaweimobileuk zoneoftech prob zareldo pro phone galaxy brand samsungus shot channel iphone\n",
      "Topic 1 | Coherence=-365.49 | Top words= phone galaxy iphone note android app new oneplus plus last camera smart mobile screen year\n",
      "Average topic coherence for the top words is -312.80393944350476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coherence': [-260.11309567426633, -365.49478321274324],\n",
       " 'top_words': [array(['samsungmobileus', 'samsunguk', 'huaweiuk', 'huaweimobileuk',\n",
       "         'zoneoftech', 'prob', 'zareldo', 'pro', 'phone', 'galaxy', 'brand',\n",
       "         'samsungus', 'shot', 'channel', 'iphone'], dtype='<U15'),\n",
       "  array(['phone', 'galaxy', 'iphone', 'note', 'android', 'app', 'new',\n",
       "         'oneplus', 'plus', 'last', 'camera', 'smart', 'mobile', 'screen',\n",
       "         'year'], dtype='<U15')]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(cv.get_feature_names())\n",
    "biterms = vec_to_biterms(samsung_cv)\n",
    "\n",
    "btm = oBTM(num_topics=2, V=vocab)\n",
    "for i in range(0, len(biterms), 100): # prozess chunk of 200 texts\n",
    "    biterms_chunk = biterms[i:i + 100]\n",
    "    btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    \n",
    "print(\"\\n\\n Topic coherence ..\")\n",
    "topic_summuary(btm.phi_wz.T, samsung_cv, vocab, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets under Topic 0:  1365\n",
      "Number of tweets under Topic 1:  1073\n"
     ]
    }
   ],
   "source": [
    "count_0=0\n",
    "for i in range(len(tweet_samsung_words)):\n",
    "    if topics[i].argmax()==0:\n",
    "        count_0+=1\n",
    "print('Number of tweets under Topic 0: ', count_0)\n",
    "count_1=0\n",
    "for i in range(len(tweet_samsung_words)):\n",
    "    if topics[i].argmax()==1:\n",
    "        count_1+=1\n",
    "print('Number of tweets under Topic 1: ', count_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_apple_clean=tweet_combined_clean[tweet_combined_clean['brand']==1]\n",
    "tweet_apple_words=tweet_apple_clean['text'].apply(text_processer)\n",
    "apple_vectorized=cv.fit_transform(tweet_apple_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 85.12it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 57.71it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 75.70it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 83.56it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 100.07it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 86.73it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 111.65it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 90.66it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 115.51it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 109.94it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 89.69it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 59.90it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 66.67it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 86.89it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 61.59it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 97.34it/s] \n",
      "100%|██████████| 50/50 [00:00<00:00, 116.32it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 121.98it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 113.68it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 156.67it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 68.48it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 88.26it/s] \n",
      "100%|██████████| 50/50 [00:00<00:00, 706.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Topic coherence ..\n",
      "Topic 0 | Coherence=-330.54 | Top words= music microsoft help app download store iamtbotouch give iphone tim google podcast cook pro itunes\n",
      "Topic 1 | Coherence=-340.13 | Top words= music spotify song app year phone back playlist tidal new video first store last lot\n",
      "Average topic coherence for the top words is -335.3335452035808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coherence': [-330.53707906334904, -340.1300113438126],\n",
       " 'top_words': [array(['music', 'microsoft', 'help', 'app', 'download', 'store',\n",
       "         'iamtbotouch', 'give', 'iphone', 'tim', 'google', 'podcast',\n",
       "         'cook', 'pro', 'itunes'], dtype='<U15'),\n",
       "  array(['music', 'spotify', 'song', 'app', 'year', 'phone', 'back',\n",
       "         'playlist', 'tidal', 'new', 'video', 'first', 'store', 'last',\n",
       "         'lot'], dtype='<U15')]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(cv.get_feature_names())\n",
    "biterms = vec_to_biterms(apple_vectorized)\n",
    "\n",
    "btm = oBTM(num_topics=2, V=vocab)\n",
    "for i in range(0, len(biterms), 100): # prozess chunk of 200 texts\n",
    "    biterms_chunk = biterms[i:i + 100]\n",
    "    btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    \n",
    "print(\"\\n\\n Topic coherence ..\")\n",
    "topic_summuary(btm.phi_wz.T, apple_vectorized, vocab, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets under Topic 0:  1726\n",
      "Number of tweets under Topic 1:  485\n"
     ]
    }
   ],
   "source": [
    "count_0=0\n",
    "for i in range(len(tweet_apple_words)):\n",
    "    if topics[i].argmax()==0:\n",
    "        count_0+=1\n",
    "print('Number of tweets under Topic 0: ', count_0)\n",
    "count_1=0\n",
    "for i in range(len(tweet_apple_words)):\n",
    "    if topics[i].argmax()==1:\n",
    "        count_1+=1\n",
    "print('Number of tweets under Topic 1: ', count_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huawei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_huawei_clean=tweet_combined_clean[tweet_combined_clean['brand']==2]\n",
    "tweet_huawei_words=tweet_huawei_clean['text'].apply(text_processer)\n",
    "huawei_vectorized=cv.fit_transform(tweet_huawei_words).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 29.95it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.49it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 24.44it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 33.53it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 17.89it/s]\n",
      "100%|██████████| 50/50 [00:05<00:00,  8.60it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 29.68it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 26.63it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 35.38it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 43.87it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.71it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.33it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 26.52it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 26.68it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 29.69it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 24.16it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 28.55it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 35.33it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 18.83it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.00it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 26.13it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 32.23it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 29.67it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 35.33it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 4558.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Topic coherence ..\n",
      "Topic 0 | Coherence=-262.76 | Top words= china ccp chinese red claw connection dragon expose focus globalnews dmhdpxfuci ttsampaio mask communist chinadaily\n",
      "Topic 1 | Coherence=-374.29 | Top words= china company chinese canada europe people mask leeszla world pro globalnews phone ccp mike psyberchic\n",
      "Average topic coherence for the top words is -318.529739420865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coherence': [-262.76479176544495, -374.2946870762851],\n",
       " 'top_words': [array(['china', 'ccp', 'chinese', 'red', 'claw', 'connection', 'dragon',\n",
       "         'expose', 'focus', 'globalnews', 'dmhdpxfuci', 'ttsampaio', 'mask',\n",
       "         'communist', 'chinadaily'], dtype='<U15'),\n",
       "  array(['china', 'company', 'chinese', 'canada', 'europe', 'people',\n",
       "         'mask', 'leeszla', 'world', 'pro', 'globalnews', 'phone', 'ccp',\n",
       "         'mike', 'psyberchic'], dtype='<U15')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(cv.get_feature_names())\n",
    "biterms = vec_to_biterms(huawei_vectorized)\n",
    "\n",
    "btm = oBTM(num_topics=2, V=vocab)\n",
    "for i in range(0, len(biterms), 100): # prozess chunk of 200 texts\n",
    "    biterms_chunk = biterms[i:i + 100]\n",
    "    btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    \n",
    "print(\"\\n\\n Topic coherence ..\")\n",
    "topic_summuary(btm.phi_wz.T, huawei_vectorized, vocab, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets under Topic 0:  1198\n",
      "Number of tweets under Topic 1:  1206\n"
     ]
    }
   ],
   "source": [
    "count_0=0\n",
    "for i in range(len(tweet_huawei_words)):\n",
    "    if topics[i].argmax()==0:\n",
    "        count_0+=1\n",
    "print('Number of tweets under Topic 0: ', count_0)\n",
    "count_1=0\n",
    "for i in range(len(tweet_huawei_words)):\n",
    "    if topics[i].argmax()==1:\n",
    "        count_1+=1\n",
    "print('Number of tweets under Topic 1: ', count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
